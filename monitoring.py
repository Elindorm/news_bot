# monitoring.py (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è 350+ –±–∞–Ω–∫–æ–≤ –Ω–∞ 4 vCPU / 8GB RAM)
import asyncio
import sqlite3
import logging
from datetime import datetime, timedelta
import pytz
from config import *
from news_analyzer import analyze_all_news, deduplicate_in_parallel, is_duplicate, calculate_informativeness
from utils import normalize_text_for_aliases, DB_WRITE_LOCK
from aiogram import Bot
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
import aiohttp
import random
import os
import json
import hashlib
import feedparser
from bs4 import BeautifulSoup
from telethon import TelegramClient
from telethon.errors import FloodWaitError, UnauthorizedError
from playwright.async_api import async_playwright
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import re
from collections import OrderedDict, defaultdict

# –ò–º–ø–æ—Ä—Ç –ø—É–ª–∞ —Å–µ—Å—Å–∏–π –∏–∑ news_parser.py
from news_parser import SESSION_POOL_LOCK, get_session_for_task, release_session

# –•—Ä–∞–Ω–∏–ª–∏—â–µ "–≥–æ—Ä—è—á–∏—Ö" –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
hot_news_cache = {}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/monitoring.log", mode="a", encoding="utf-8")
    ]
)

# === –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –ü–û–î 4 vCPU / 8GB RAM ===
BANK_SEM = asyncio.Semaphore(2)          # –î–æ 2 –±–∞–Ω–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
RSS_SEM = asyncio.Semaphore(5)           # –î–æ 5 RSS-–ª–µ–Ω—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
PLAYWRIGHT_SEM = asyncio.Semaphore(1)    # Playwright –æ—Å—Ç–∞—ë—Ç—Å—è 1 (—Ç—è–∂—ë–ª—ã–π)
HTTP_LIMIT = 50
BATCH_SIZE = 10
DELAY_BETWEEN_BANKS = 2
DELAY_BETWEEN_BATCHES = 15
ACTIVE_SUBSCRIPTION_DAYS = 30

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def init_monitoring_db():
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS subscriptions (
                chat_id INTEGER,
                bank_name TEXT,
                reg_number TEXT,
                last_notification TIMESTAMP DEFAULT '1970-01-01 00:00:00',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (chat_id, bank_name)
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS monitored_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                bank_name TEXT,
                reg_number TEXT,
                text TEXT,
                date TEXT,
                link TEXT,
                source TEXT,
                topic TEXT DEFAULT '',
                is_monitoring BOOLEAN DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE (link, bank_name)
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analyzed_monitored_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                bank_name TEXT,
                reg_number TEXT,
                text TEXT,
                summary TEXT,
                event_type TEXT,
                event_date TEXT,
                entities TEXT,
                date TEXT,
                link TEXT,
                source TEXT,
                category TEXT,
                sentiment TEXT,
                informativeness INTEGER,
                summary_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE (link, bank_name)
            )
        ''')
        conn.commit()
        logging.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö monitoring.db –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ monitoring.db: {e}")
    finally:
        conn.close()

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
async def save_to_monitoring_db_async(data, table_name="monitored_news"):
    if not data:
        return
    async with DB_WRITE_LOCK:
        try:
            conn = sqlite3.connect('monitoring.db')
            cursor = conn.cursor()
            inserted_count = 0
            if table_name == "monitored_news":
                for item in data:
                    cursor.execute('''
                        INSERT OR IGNORE INTO monitored_news (bank_name, reg_number, text, date, link, source, topic, is_monitoring)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        item.get("bank", ""),
                        item.get("reg_number", ""),
                        item.get("text", ""),
                        item.get("date", ""),
                        item.get("link", ""),
                        item.get("source", ""),
                        item.get("topic", ""),
                        1
                    ))
                    if cursor.rowcount > 0:
                        inserted_count += 1
            elif table_name == "analyzed_monitored_news":
                for item in data:
                    summary_hash = hashlib.md5(item.get("summary", "").encode('utf-8')).hexdigest()
                    cursor.execute('''
                        INSERT OR IGNORE INTO analyzed_monitored_news (
                            bank_name, reg_number, text, summary, event_type, event_date,
                            entities, date, link, source, category, sentiment, informativeness, summary_hash
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        item.get("bank", ""),
                        item.get("reg_number", ""),
                        item.get("text", ""),
                        item.get("summary", ""),
                        item.get("event_type", ""),
                        item.get("event_date", "").strftime("%Y-%m-%d") if isinstance(item.get("event_date"), datetime) else item.get("event_date", ""),
                        json.dumps(item.get("entities", [])),
                        item.get("date", ""),
                        item.get("link", ""),
                        item.get("source", ""),
                        item.get("category", ""),
                        item.get("sentiment", ""),
                        item.get("informativeness", 0),
                        summary_hash
                    ))
                    if cursor.rowcount > 0:
                        inserted_count += 1
            conn.commit()
            logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {inserted_count} –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ {table_name}")
        except sqlite3.Error as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {table_name}: {e}")
        finally:
            conn.close()

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∫–∞–º–∏
def add_subscription(chat_id, bank_name):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        reg_number = BANKS.get(bank_name, {}).get("reg_number", bank_name)
        cursor.execute('''
            INSERT OR IGNORE INTO subscriptions (chat_id, bank_name, reg_number, last_notification)
            VALUES (?, ?, ?, '1970-01-01 00:00:00')
        ''', (chat_id, bank_name, reg_number))
        conn.commit()
        logging.info(f"–ü–æ–¥–ø–∏—Å–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: chat_id={chat_id}, bank={bank_name}.")
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–∫–∏: {e}")
    finally:
        conn.close()

def remove_subscription(chat_id, bank_name):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('DELETE FROM subscriptions WHERE chat_id = ? AND bank_name = ?', (chat_id, bank_name))
        conn.commit()
        logging.info(f"–ü–æ–¥–ø–∏—Å–∫–∞ —É–¥–∞–ª–µ–Ω–∞: chat_id={chat_id}, bank={bank_name}")
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–∫–∏: {e}")
    finally:
        conn.close()

def get_user_subscriptions(chat_id):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('SELECT bank_name, reg_number FROM subscriptions WHERE chat_id = ?', (chat_id,))
        return cursor.fetchall()
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–æ–∫ –¥–ª—è chat_id {chat_id}: {e}")
        return []
    finally:
        conn.close()

def get_user_subscriptions_by_bank(bank_name):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('SELECT chat_id FROM subscriptions WHERE bank_name = ?', (bank_name,))
        rows = cursor.fetchall()
        return [row[0] for row in rows]
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–æ–∫ –¥–ª—è {bank_name}: {e}")
        return []
    finally:
        conn.close()

def get_all_subscriptions():
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('SELECT chat_id, bank_name FROM subscriptions')
        return cursor.fetchall()
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–¥–ø–∏—Å–æ–∫: {e}")
        return []
    finally:
        conn.close()

def get_active_banks(days=ACTIVE_SUBSCRIPTION_DAYS):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
        cursor.execute('''
            SELECT DISTINCT bank_name 
            FROM subscriptions 
            WHERE created_at >= ? OR last_notification >= ?
        ''', (cutoff_date, cutoff_date))
        banks = [row[0] for row in cursor.fetchall()]
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(banks)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞–Ω–∫–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π)")
        return banks
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞–Ω–∫–æ–≤: {e}")
        return []
    finally:
        conn.close()

def get_new_analyzed_news(chat_id):
    conn = None
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('SELECT bank_name FROM subscriptions WHERE chat_id = ?', (chat_id,))
        banks = [row[0] for row in cursor.fetchall()]
        if not banks:
            return []
        cursor.execute('SELECT last_notification FROM subscriptions WHERE chat_id = ? LIMIT 1', (chat_id,))
        last_notif_row = cursor.fetchone()
        last_notification = last_notif_row[0] if last_notif_row else '1970-01-01 00:00:00'
        placeholders = ','.join(['?'] * len(banks))
        cursor.execute(f'''
            SELECT bank_name, reg_number, text, summary, event_type, event_date, entities, 
                   date, link, source, category, sentiment, informativeness
            FROM analyzed_monitored_news
            WHERE bank_name IN ({placeholders}) AND created_at > ?
            ORDER BY created_at DESC
        ''', banks + [last_notification])
        rows = cursor.fetchall()
        news_list = []
        for row in rows:
            news_item = {
                "bank": row[0],
                "reg_number": row[1],
                "text": row[2],
                "summary": row[3],
                "event_type": row[4],
                "event_date": row[5],
                "entities": json.loads(row[6]) if row[6] else [],
                "date": row[7],
                "link": row[8],
                "source": row[9],
                "category": row[10],
                "sentiment": row[11],
                "informativeness": row[12]
            }
            news_list.append(news_item)
        logging.info(f"–î–ª—è chat_id={chat_id} –Ω–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤—ã—Ö analyzed –Ω–æ–≤–æ—Å—Ç–µ–π")
        return news_list
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è chat_id {chat_id}: {e}")
        return []
    finally:
        if conn:
            conn.close()

def update_last_notification(chat_id):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('UPDATE subscriptions SET last_notification = CURRENT_TIMESTAMP WHERE chat_id = ?', (chat_id,))
        conn.commit()
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è last_notification –¥–ª—è {chat_id}: {e}")
    finally:
        conn.close()

# === –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê ===
async def fetch_1000bankov_news_monitoring(bank_name, date_from, date_to):
    reg_number = BANKS.get(bank_name, {}).get("reg_number", bank_name)
    aliases = generate_aliases(bank_name)
    news_data = []
    date_from_dt = datetime.strptime(date_from, "%Y-%m-%d").date()
    date_to_dt = datetime.strptime(date_to, "%Y-%m-%d").date()
    async with PLAYWRIGHT_SEM:
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-extensions',
                        '--disable-plugins',
                        '--disable-images',
                        '--blink-settings=imagesEnabled=false'
                    ]
                )
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    viewport={'width': 1920, 'height': 1080},
                    java_script_enabled=True,
                    ignore_https_errors=True
                )
                page = await context.new_page()
                url = f"https://1000bankov.ru/news/bank/{reg_number}/"
                logging.info(f"Playwright: –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ {url} –¥–ª—è {bank_name}")
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_timeout(1000)
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                news_cards = soup.find_all('div', class_='newsCard')
                logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_cards)} –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {bank_name}")
                for card in news_cards:
                    try:
                        title_elem = card.find('h3', class_='newsCard__header')
                        link_elem = card.find('a', class_='newsCard__headerLink')
                        date_elem = card.find('span', class_='newsCard__date')
                        if not (title_elem and link_elem and date_elem):
                            continue
                        title = title_elem.text.strip()
                        link = link_elem['href']
                        full_link = link if link.startswith('http') else f"https://1000bankov.ru{link}"
                        date_str = date_elem.text.strip()
                        try:
                            date_obj = datetime.strptime(date_str, "%d.%m.%Y")
                            news_date = date_obj.strftime("%Y-%m-%d")
                            news_date_dt = date_obj.date()
                        except ValueError:
                            continue
                        if not (date_from_dt <= news_date_dt <= date_to_dt):
                            continue
                        if is_bank_name_match(title, aliases):
                            news_data.append({
                                "bank": bank_name,
                                "reg_number": reg_number,
                                "text": title,
                                "date": news_date,
                                "link": full_link,
                                "source": "1000bankov.ru"
                            })
                    except Exception as e:
                        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è {bank_name}: {e}")
                await browser.close()
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ Playwright –¥–ª—è {bank_name}: {e}")
        finally:
            await asyncio.sleep(3)
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_data)} –Ω–æ–≤–æ—Å—Ç–µ–π —Å 1000bankov –¥–ª—è {bank_name}")
    return news_data

async def fetch_telegram_news_monitoring(bank_name, date_from, date_to):
    reg_number = BANKS.get(bank_name, {}).get("reg_number", bank_name)
    aliases = generate_aliases(bank_name)
    all_messages = []
    session_info = await get_session_for_task(is_monitoring=True)
    if not session_info:
        logging.warning(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–µ—Å—Å–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Telegram –¥–ª—è {bank_name}")
        return []
    client = None
    try:
        account_idx = int(session_info["name"].split("_")[1])
        account = ACCOUNTS[account_idx]
        client = TelegramClient(f"sessions/{session_info['name']}", account["api_id"], account["api_hash"])
        await client.connect()
        client.session._execute('PRAGMA busy_timeout = 5000')
        if not await client.is_user_authorized():
            logging.error(f"–°–µ—Å—Å–∏—è {session_info['name']} –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞.")
            try:
                os.remove(f"sessions/{session_info['name']}.session")
            except Exception:
                pass
            return []
        
        for channel in NEWS_CHANNELS:
            for _attempt in range(3):  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                try:
                    async for message in client.iter_messages(channel.lower(), limit=25):
                        if message.text and message.date:
                            message_date = message.date.replace(tzinfo=None).date()
                            date_from_dt = datetime.strptime(date_from, "%Y-%m-%d").date()
                            date_to_dt = datetime.strptime(date_to, "%Y-%m-%d").date()
                            if date_from_dt <= message_date <= date_to_dt and is_bank_name_match(message.text, aliases):
                                all_messages.append({
                                    "bank": bank_name,
                                    "reg_number": reg_number,
                                    "text": message.text,
                                    "date": message_date.strftime("%Y-%m-%d"),
                                    "link": f"https://t.me/{channel}/{message.id}",  # –£–±—Ä–∞–ª–∏ –ø—Ä–æ–±–µ–ª
                                    "source": f"telegram_{channel}"
                                })
                    break
                except sqlite3.OperationalError as e:
                    if 'database is locked' in str(e).lower():
                        await asyncio.sleep(2)
                    else:
                        raise
                except FloodWaitError as e:
                    await asyncio.sleep(e.seconds + random.uniform(0, 2))
                except UnauthorizedError:
                    break
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–Ω–∞–ª–∞ {channel}: {e}")
                    break
        logging.info(f"Telegram: –Ω–∞–π–¥–µ–Ω–æ {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è {bank_name}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Telegram –¥–ª—è {bank_name}: {e}")
    finally:
        release_session(session_info)
        if client and client.is_connected():
            await client.disconnect()
    return all_messages

async def scrape_inkazan_news_monitoring(session, bank_name, aliases, date_from, date_to):
    url = "https://inkazan.ru/news"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    reg_number = BANKS.get(bank_name, {}).get("reg_number", bank_name)
    articles = []
    try:
        async with session.get(url, headers=headers) as response:
            if response.status != 200:
                return []
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            news_items = soup.select('div.news-list__item')
            for item in news_items:
                title_elem = item.select_one('a.news-list__title')
                if not title_elem:
                    continue
                title = title_elem.get_text(strip=True)
                link = title_elem['href']
                link = link if link.startswith('http') else f"https://inkazan.ru{link}"
                date_elem = item.select_one('div.news-list__date')
                date_str = date_elem.get_text(strip=True) if date_elem else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                try:
                    date_match = re.search(r'(\d{1,2})\s+(\w+)\s*(\d{4})', date_str)
                    if date_match:
                        day = int(date_match.group(1))
                        month_name = date_match.group(2).lower()
                        year = int(date_match.group(3))
                        months = {'—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12}
                        month = months.get(month_name)
                        if month:
                            news_date = datetime(year, month, day).strftime("%Y-%m-%d")
                        else:
                            continue
                    else:
                        continue
                    news_date_dt = datetime.strptime(news_date, "%Y-%m-%d").date()
                    date_from_dt = datetime.strptime(date_from, "%Y-%m-%d").date()
                    date_to_dt = datetime.strptime(date_to, "%Y-%m-%d").date()
                    if not (date_from_dt <= news_date_dt <= date_to_dt):
                        continue
                except Exception:
                    continue
                async with session.get(link, headers=headers) as article_response:
                    if article_response.status != 200:
                        continue
                    article_html = await article_response.text()
                    article_soup = BeautifulSoup(article_html, 'html.parser')
                    content_elem = article_soup.select_one('div.article__content')
                    text = content_elem.get_text(separator=" ", strip=True) if content_elem else title
                    if text and is_bank_name_match(text, aliases):
                        articles.append({
                            "bank": bank_name,
                            "reg_number": reg_number,
                            "text": text,
                            "date": news_date,
                            "link": link,
                            "source": "inkazan.ru"
                        })
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ inkazan.ru: {e}")
    return articles

async def fetch_rss_news_monitoring(bank_name, date_from, date_to):
    reg_number = BANKS.get(bank_name, {}).get("reg_number", bank_name)
    aliases = generate_aliases(bank_name)
    all_articles = []
    connector = aiohttp.TCPConnector(limit=HTTP_LIMIT)
    async with aiohttp.ClientSession(connector=connector) as session:
        inkazan_task = scrape_inkazan_news_monitoring(session, bank_name, aliases, date_from, date_to)
        rss_tasks = []
        for rss_feed in RSS_FEEDS:
            async def limited_rss_parse(feed=rss_feed):
                async with RSS_SEM:
                    return await parse_single_rss_feed_monitoring(session, feed, bank_name, reg_number, aliases, date_from, date_to)
            rss_tasks.append(asyncio.create_task(limited_rss_parse()))
        results = await asyncio.gather(inkazan_task, *rss_tasks, return_exceptions=True)
        for result in results:
            if isinstance(result, Exception):
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ RSS –∏–ª–∏ inkazan: {result}")
                continue
            elif result:
                all_articles.extend(result)
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(all_articles)} RSS-–Ω–æ–≤–æ—Å—Ç–µ–π (–≤–∫–ª—é—á–∞—è inkazan.ru) –¥–ª—è {bank_name}")
    return all_articles

async def parse_single_rss_feed_monitoring(session, rss_feed, bank_name, reg_number, aliases, date_from, date_to):
    logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ RSS-–ª–µ–Ω—Ç—ã: {rss_feed}")
    articles = []
    try:
        async with session.get(rss_feed) as response:
            if response.status != 200:
                logging.warning(f"RSS-–ª–µ–Ω—Ç–∞ {rss_feed} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: HTTP {response.status}")
                return articles
            feed_text = await response.text()
            feed = feedparser.parse(feed_text)
            if not feed.entries:
                logging.warning(f"–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –≤ RSS-–ª–µ–Ω—Ç–µ: {rss_feed}")
                return articles
            for entry in feed.entries:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        entry_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        entry_date = datetime(*entry.updated_parsed[:6])
                    else:
                        continue
                    date_str = entry_date.strftime("%Y-%m-%d")
                    entry_date_dt = entry_date.date()
                    date_from_dt = datetime.strptime(date_from, "%Y-%m-%d").date()
                    date_to_dt = datetime.strptime(date_to, "%Y-%m-%d").date()
                    if not (date_from_dt <= entry_date_dt <= date_to_dt):
                        continue
                    title = entry.get('title', '')
                    summary = entry.get('summary', '')
                    content = entry.get('content', [{}])
                    if content and isinstance(content, list) and 'value' in content[0]:
                        content_text = content[0]['value']
                    else:
                        content_text = ''
                    text = f"{title} {summary} {content_text}".strip()
                    if not text:
                        continue
                    if is_bank_name_match(text, aliases):
                        articles.append({
                            "bank": bank_name,
                            "reg_number": reg_number,
                            "text": text,
                            "date": date_str,
                            "link": entry.link,
                            "source": rss_feed
                        })
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ RSS –≤ –ª–µ–Ω—Ç–µ {rss_feed}: {e}")
                    continue
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ RSS-–ª–µ–Ω—Ç–µ {rss_feed}: {e}")
    return articles

def generate_aliases(bank_name):
    aliases = [bank_name]
    if bank_name in BANKS:
        aliases.extend(BANKS[bank_name].get("aliases", []))
    return aliases

def is_bank_name_match(text, aliases):
    if not text:
        return False
    normalized_text = normalize_text_for_aliases(text)
    for alias in aliases:
        normalized_alias = normalize_text_for_aliases(alias)
        alias_words = normalized_alias.split()
        all_words_found = all(word in normalized_text for word in alias_words)
        if all_words_found:
            return True
    return False

def is_news_already_in_db(link, bank_name, table_name="analyzed_monitored_news"):
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute(f'SELECT 1 FROM {table_name} WHERE link = ? AND bank_name = ? LIMIT 1', (link, bank_name))
        return cursor.fetchone() is not None
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞ –¥–ª—è link={link}, bank={bank_name}: {e}")
        return False
    finally:
        conn.close()

def get_existing_analyzed_summaries(bank_name, days=30):
    last_date = datetime.now() - timedelta(days=days)
    try:
        conn = sqlite3.connect('monitoring.db')
        cursor = conn.cursor()
        cursor.execute('''
            SELECT summary, date, event_type, entities, category FROM analyzed_monitored_news 
            WHERE bank_name = ? AND created_at > ?
        ''', (bank_name, last_date.strftime("%Y-%m-%d %H:%M:%S")))
        rows = cursor.fetchall()
        return [(row[0], row[1], row[2], row[3], row[4]) for row in rows]
    except sqlite3.Error as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö summaries –¥–ª—è {bank_name}: {e}")
        return []
    finally:
        conn.close()

# --- –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò –ë–ê–ù–ö–ê ---
async def process_bank_monitoring(bank_name, date_from, date_to):
    all_news = []
    rss_news = await fetch_rss_news_monitoring(bank_name, date_from, date_to)
    all_news.extend(rss_news)
    await asyncio.sleep(1)
    bankov_news = await fetch_1000bankov_news_monitoring(bank_name, date_from, date_to)
    all_news.extend(bankov_news)
    await asyncio.sleep(1)
    telegram_news = await fetch_telegram_news_monitoring(bank_name, date_from, date_to)
    all_news.extend(telegram_news)
    await asyncio.sleep(1)
    filtered_news = [item for item in all_news if not is_news_already_in_db(item.get("link", ""), bank_name, "monitored_news")]
    if not filtered_news:
        logging.info(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö raw –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {bank_name}")
        return []
    await save_to_monitoring_db_async(filtered_news, "monitored_news")
    analyzed_news = await analyze_all_news(filtered_news, topic=None, is_monitoring=True)
    if not analyzed_news:
        logging.info(f"–ü–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {bank_name}")
        return []
    existing_summaries = get_existing_analyzed_summaries(bank_name)
    if not existing_summaries:
        async with aiohttp.ClientSession() as session:
            semaphore = asyncio.Semaphore(5)
            unique_news = await deduplicate_in_parallel(analyzed_news, session, semaphore, similarity_threshold=0.85)
    else:
        combined_news = []
        for summary, date_str, event_type, entities_str, category in existing_summaries:
            combined_news.append({
                "summary": summary,
                "date": date_str,
                "event_type": event_type,
                "entities": [e.strip() for e in entities_str.split(',')] if entities_str else [],
                "source": "database",
                "informativeness": calculate_informativeness(summary),
                "category": category or "–û–±—ã—á–Ω–∞—è",
                "is_from_db": True
            })
        for news in analyzed_news:
            combined_news.append({**news, "is_from_db": False})
        async with aiohttp.ClientSession() as session:
            semaphore = asyncio.Semaphore(5)
            unique_combined = await deduplicate_in_parallel(combined_news, session, semaphore, similarity_threshold=0.85)
        unique_news = [news for news in unique_combined if not news.get("is_from_db", False)]
    if not unique_news:
        logging.info(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö analyzed –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {bank_name}")
        return []
    await save_to_monitoring_db_async(unique_news, "analyzed_monitored_news")
    return unique_news

async def monitoring_loop(bot):
    init_monitoring_db()
    scheduled_hours = [7, 11, 15, 19]
    moscow_tz = pytz.timezone('Europe/Moscow')

    while True:
        try:
            now = datetime.now(moscow_tz)
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–ø—É—Å–∫
            next_run = None
            for hour in scheduled_hours:
                candidate = now.replace(hour=hour, minute=0, second=0, microsecond=0)
                if candidate > now:
                    if next_run is None or candidate < next_run:
                        next_run = candidate
            if next_run is None:
                next_run = now.replace(hour=scheduled_hours[0], minute=0, second=0, microsecond=0) + timedelta(days=1)

            sleep_seconds = (next_run - now).total_seconds()
            if sleep_seconds > 0:
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞ –¥–æ {next_run.strftime('%Y-%m-%d %H:%M:%S')} ({int(sleep_seconds)} —Å–µ–∫)")
                await asyncio.sleep(sleep_seconds)

            # === –ó–ê–ü–£–°–ö –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê ===
            logging.info("‚úÖ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            run_time = datetime.now(moscow_tz)
            date_to = run_time.strftime("%Y-%m-%d")
            date_from = (run_time - timedelta(hours=12)).strftime("%Y-%m-%d")
            banks = get_active_banks()
            if not banks:
                logging.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞–Ω–∫–æ–≤ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ü–∏–∫–ª.")
                continue

            user_notifications = defaultdict(lambda: defaultdict(list))
            for i in range(0, len(banks), BATCH_SIZE):
                batch = banks[i:i + BATCH_SIZE]
                logging.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i // BATCH_SIZE + 1}: {len(batch)} –±–∞–Ω–∫–æ–≤")

                tasks = []
                for bank in batch:
                    async def process_with_semaphore(b_name):
                        async with BANK_SEM:
                            result = await process_bank_monitoring(b_name, date_from, date_to)
                            await asyncio.sleep(DELAY_BETWEEN_BANKS)
                            return result
                    tasks.append(asyncio.create_task(process_with_semaphore(bank)))

                results = await asyncio.gather(*tasks, return_exceptions=True)
                for j, result in enumerate(results):
                    bank_name = batch[j]
                    if isinstance(result, Exception):
                        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞–Ω–∫–∞ {bank_name}: {result}")
                        continue
                    if result:
                        subs = get_user_subscriptions_by_bank(bank_name)
                        for chat_id in subs:
                            user_notifications[chat_id][bank_name].extend(result)

                await asyncio.sleep(DELAY_BETWEEN_BATCHES)

            # === –û–¢–ü–†–ê–í–ö–ê –£–í–ï–î–û–ú–õ–ï–ù–ò–ô ===
            all_subscriptions = get_all_subscriptions()
            unique_chats = {chat_id for chat_id, _ in all_subscriptions}

            for chat_id in unique_chats:
                bank_news = user_notifications[chat_id]
                total = sum(len(news) for news in bank_news.values())
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                monitoring_iteration_id = f"{int(datetime.now().timestamp())}_{chat_id}"
                
                if total > 0:
                    message = "üì¨ <b>–ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤–∞—à–∏–º –ø–æ–¥–ø–∏—Å–∫–∞–º!</b>\n"
                    for bank, news_list in bank_news.items():
                        neg = sum(1 for n in news_list if n.get("sentiment") == "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è")
                        message += f"‚Ä¢ {bank}: {len(news_list)} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö (üî¥ {neg} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö)\n"
                    message += f"\n–í—Å–µ–≥–æ: {total}\n–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å."
                    
                    keyboard = InlineKeyboardMarkup(inline_keyboard=[
                        [InlineKeyboardButton(text="üì∞ –ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏", callback_data=f"view_monitoring_iteration_{monitoring_iteration_id}")]
                    ])
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                    if chat_id not in hot_news_cache:
                        hot_news_cache[chat_id] = {}
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                    iteration_news = []
                    for news_list in bank_news.values():
                        iteration_news.extend(news_list)
                    
                    hot_news_cache[chat_id][monitoring_iteration_id] = {
                        "news": iteration_news,
                        "timestamp": datetime.now().timestamp()
                    }
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–µ—à–∞ - —Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
                    if len(hot_news_cache[chat_id]) > 10:
                        # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏
                        oldest_iterations = sorted(
                            hot_news_cache[chat_id].items(), 
                            key=lambda x: x[1]["timestamp"]
                        )[:len(hot_news_cache[chat_id]) - 10]
                        for iter_id, _ in oldest_iterations:
                            del hot_news_cache[chat_id][iter_id]
                    
                else:
                    message = "üì≠ <b>–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —á–∞—Å–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–∏–º –ø–æ–¥–ø–∏—Å–∫–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.</b>\n–ú—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥."
                    keyboard = InlineKeyboardMarkup(inline_keyboard=[
                        [InlineKeyboardButton(text="üîç –ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∞—Ä—Ö–∏–≤ –Ω–æ–≤–æ—Å—Ç–µ–π", callback_data="view_monitoring_archive")],
                        [InlineKeyboardButton(text="üè† –í –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="return_to_main_menu")]
                    ])

                try:
                    await bot.send_message(chat_id, message, parse_mode="HTML", reply_markup=keyboard, disable_web_page_preview=True)
                    if total > 0:
                        update_last_notification(chat_id)
                except Exception as e:
                    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ chat_id={chat_id}: {e}")

        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ monitoring_loop: {e}", exc_info=True)
            await asyncio.sleep(60)